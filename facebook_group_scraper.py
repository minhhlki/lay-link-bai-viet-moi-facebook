#!/usr/bin/env python3
"""
Facebook Group Post Scraper
L·∫•y t·∫•t c·∫£ links b√†i vi·∫øt t·ª´ Facebook group trong tu·∫ßn qua
"""

import asyncio
import re
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Set
from playwright.async_api import async_playwright, Page, Browser
from colorama import Fore, Style, init

# Initialize colorama
init(autoreset=True)


class FacebookGroupScraper:
    def __init__(self, headless: bool = False, user_data_dir: str = None):
        """
        Initialize the scraper

        Args:
            headless: Ch·∫°y browser ·∫©n (True) ho·∫∑c hi·ªán (False)
            user_data_dir: Th∆∞ m·ª•c l∆∞u cookies/session ƒë·ªÉ kh√¥ng ph·∫£i login l·∫°i
        """
        self.headless = headless
        self.user_data_dir = user_data_dir or "./browser_data"
        self.browser: Browser = None
        self.page: Page = None
        self.context = None
        self.playwright = None

    async def init_browser(self):
        """Kh·ªüi t·∫°o browser v·ªõi Playwright"""
        print(f"{Fore.CYAN}üöÄ ƒêang kh·ªüi ƒë·ªông browser...")

        try:
            self.playwright = await async_playwright().start()

            # S·ª≠ d·ª•ng persistent context ƒë·ªÉ l∆∞u cookies/session
            self.context = await self.playwright.chromium.launch_persistent_context(
                self.user_data_dir,
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                ],
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )

            self.page = await self.context.new_page()
            print(f"{Fore.GREEN}‚úì Browser ƒë√£ s·∫µn s√†ng")

        except Exception as e:
            error_msg = str(e)

            # Check for missing dependencies error
            if "missing dependencies" in error_msg.lower():
                print(f"\n{Fore.RED}‚úó L·ªói: Thi·∫øu system dependencies ƒë·ªÉ ch·∫°y browser")
                print(f"\n{Fore.YELLOW}Gi·∫£i ph√°p:")
                print(f"{Fore.YELLOW}1. Ch·∫°y l·ªánh sau (c·∫ßn sudo):")
                print(f"{Fore.WHITE}   sudo playwright install-deps")
                print(f"\n{Fore.YELLOW}2. Ho·∫∑c c√†i th·ªß c√¥ng:")
                print(f"{Fore.WHITE}   sudo apt-get install libnss3 libnspr4 libgbm1")
                print(f"\n{Fore.YELLOW}3. N·∫øu kh√¥ng c√≥ sudo access (VD: tr√™n JupyterLab):")
                print(f"{Fore.WHITE}   - Th·ª≠ ch·∫°y v·ªõi headless=False (ch·ªçn 'n' khi h·ªèi)")
                print(f"{Fore.WHITE}   - Ho·∫∑c li√™n h·ªá admin ƒë·ªÉ c√†i dependencies")

            raise

    async def check_login_status(self) -> bool:
        """Ki·ªÉm tra xem ƒë√£ login Facebook ch∆∞a"""
        try:
            # Ki·ªÉm tra xem c√≥ cookie Facebook kh√¥ng
            cookies = await self.context.cookies()
            fb_cookies = [c for c in cookies if 'facebook.com' in c.get('domain', '')]

            if fb_cookies:
                print(f"{Fore.GREEN}‚úì ƒê√£ c√≥ session Facebook")
                return True
            else:
                print(f"{Fore.YELLOW}‚ö† Ch∆∞a login Facebook")
                return False
        except:
            return False

    async def wait_for_login(self):
        """ƒê·ª£i user login th·ªß c√¥ng"""
        print(f"\n{Fore.YELLOW}{'='*60}")
        print(f"{Fore.YELLOW}üìù Vui l√≤ng login Facebook trong c·ª≠a s·ªï browser")
        print(f"{Fore.YELLOW}   Sau khi login xong, nh·∫•n Enter ƒë·ªÉ ti·∫øp t·ª•c...")
        print(f"{Fore.YELLOW}{'='*60}\n")

        # M·ªü Facebook ƒë·ªÉ login
        await self.page.goto('https://www.facebook.com/', wait_until='networkidle')

        # ƒê·ª£i user nh·∫•n Enter
        input(f"{Fore.CYAN}> Nh·∫•n Enter sau khi login xong: ")
        print(f"{Fore.GREEN}‚úì Ti·∫øp t·ª•c...")

    def parse_relative_time(self, time_text: str) -> datetime:
        """
        Parse th·ªùi gian t∆∞∆°ng ƒë·ªëi t·ª´ Facebook (VD: "2h", "5 ph√∫t", "3 ng√†y")

        Args:
            time_text: Text th·ªùi gian t·ª´ Facebook

        Returns:
            datetime object
        """
        now = datetime.now()
        time_text = time_text.lower().strip()

        # Pattern cho c√°c ƒë·ªãnh d·∫°ng th·ªùi gian
        patterns = {
            r'(\d+)\s*gi√¢y': ('seconds', 1),
            r'(\d+)\s*ph√∫t': ('minutes', 1),
            r'(\d+)\s*gi·ªù': ('hours', 1),
            r'(\d+)\s*h': ('hours', 1),
            r'(\d+)\s*ng√†y': ('days', 1),
            r'(\d+)\s*tu·∫ßn': ('weeks', 1),
            r'(\d+)\s*th√°ng': ('months', 30),
            r'(\d+)\s*nƒÉm': ('years', 365),
        }

        for pattern, (unit, multiplier) in patterns.items():
            match = re.search(pattern, time_text)
            if match:
                value = int(match.group(1))
                if unit in ['seconds', 'minutes', 'hours', 'days', 'weeks']:
                    delta = timedelta(**{unit: value})
                else:
                    # Cho th√°ng v√† nƒÉm, convert sang days
                    delta = timedelta(days=value * multiplier)
                return now - delta

        # N·∫øu ch·ªâ c√≥ "v√†i gi√¢y" ho·∫∑c "V·ª´a xong"
        if any(x in time_text for x in ['v·ª´a xong', 'just now', 'v√†i gi√¢y']):
            return now

        # Kh√¥ng parse ƒë∆∞·ª£c, tr·∫£ v·ªÅ th·ªùi gian hi·ªán t·∫°i
        return now

    async def scroll_and_load_posts(self, max_scrolls: int = 50):
        """
        Scroll xu·ªëng ƒë·ªÉ load th√™m posts

        Args:
            max_scrolls: S·ªë l·∫ßn scroll t·ªëi ƒëa
        """
        print(f"{Fore.CYAN}üìú ƒêang scroll v√† load posts...")

        last_height = await self.page.evaluate('document.body.scrollHeight')
        scroll_count = 0
        no_change_count = 0

        while scroll_count < max_scrolls:
            # Scroll xu·ªëng cu·ªëi
            await self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')

            # ƒê·ª£i load
            await asyncio.sleep(2)

            # Ki·ªÉm tra height m·ªõi
            new_height = await self.page.evaluate('document.body.scrollHeight')

            if new_height == last_height:
                no_change_count += 1
                if no_change_count >= 3:
                    print(f"{Fore.GREEN}‚úì ƒê√£ load h·∫øt posts")
                    break
            else:
                no_change_count = 0

            last_height = new_height
            scroll_count += 1
            print(f"{Fore.CYAN}   Scroll {scroll_count}/{max_scrolls}...", end='\r')

        print()  # New line

    async def extract_post_links(self, days: int = 7) -> List[dict]:
        """
        Extract t·∫•t c·∫£ post links trong kho·∫£ng th·ªùi gian

        Args:
            days: S·ªë ng√†y l·∫•y posts (m·∫∑c ƒë·ªãnh 7 ng√†y)

        Returns:
            List c√°c posts v·ªõi link v√† metadata
        """
        print(f"{Fore.CYAN}üîç ƒêang extract post links...")

        cutoff_date = datetime.now() - timedelta(days=days)
        posts = []
        seen_links = set()

        # L·∫•y t·∫•t c·∫£ post elements
        # Facebook d√πng nhi·ªÅu selector kh√°c nhau, th·ª≠ nhi·ªÅu c√°ch
        selectors = [
            'div[role="article"]',
            'div.x1yztbdb',  # Facebook's class cho posts
            '[data-pagelet^="FeedUnit"]',
        ]

        post_elements = []
        for selector in selectors:
            elements = await self.page.query_selector_all(selector)
            if elements:
                post_elements = elements
                print(f"{Fore.GREEN}‚úì T√¨m th·∫•y {len(elements)} posts v·ªõi selector: {selector}")
                break

        if not post_elements:
            print(f"{Fore.RED}‚úó Kh√¥ng t√¨m th·∫•y posts n√†o")
            return []

        print(f"{Fore.CYAN}üìä ƒêang ph√¢n t√≠ch {len(post_elements)} posts...")

        for idx, element in enumerate(post_elements, 1):
            try:
                # L·∫•y HTML c·ªßa post
                html = await element.inner_html()

                # T√¨m link b√†i vi·∫øt
                # Facebook post links c√≥ format: /groups/{group_id}/posts/{post_id}
                # ho·∫∑c /groups/{group_id}/permalink/{post_id}
                link_patterns = [
                    r'href="(/groups/\d+/posts/\d+)',
                    r'href="(/groups/\d+/permalink/\d+)',
                    r'href="(https://www\.facebook\.com/groups/\d+/posts/\d+)',
                    r'href="(https://www\.facebook\.com/groups/\d+/permalink/\d+)',
                ]

                post_link = None
                for pattern in link_patterns:
                    matches = re.findall(pattern, html)
                    if matches:
                        post_link = matches[0]
                        # L√†m s·∫°ch link (remove query params)
                        post_link = post_link.split('?')[0]
                        # Convert relative URL to absolute
                        if not post_link.startswith('http'):
                            post_link = f"https://www.facebook.com{post_link}"
                        break

                if not post_link or post_link in seen_links:
                    continue

                # T√¨m th·ªùi gian ƒëƒÉng
                time_element = await element.query_selector('a[href*="posts"] span, a[href*="permalink"] span')
                post_time = datetime.now()  # Default

                if time_element:
                    time_text = await time_element.inner_text()
                    post_time = self.parse_relative_time(time_text)

                # Ki·ªÉm tra xem post c√≥ trong kho·∫£ng th·ªùi gian kh√¥ng
                if post_time < cutoff_date:
                    continue

                # L·∫•y preview text c·ªßa post
                preview = ""
                text_elements = await element.query_selector_all('div[dir="auto"]')
                if text_elements:
                    for text_el in text_elements[:3]:  # Ch·ªâ l·∫•y 3 ƒëo·∫°n ƒë·∫ßu
                        text = await text_el.inner_text()
                        if len(text) > 20:  # B·ªè qua text qu√° ng·∫Øn
                            preview = text[:200]
                            break

                posts.append({
                    'link': post_link,
                    'time': post_time.isoformat(),
                    'time_relative': self.get_relative_time_string(post_time),
                    'preview': preview
                })

                seen_links.add(post_link)
                print(f"{Fore.GREEN}   ‚úì Post {len(posts)}: {post_link[:60]}...", end='\r')

            except Exception as e:
                continue

        print()  # New line
        print(f"{Fore.GREEN}‚úì T√¨m th·∫•y {len(posts)} posts trong {days} ng√†y qua")

        # Sort by time (newest first)
        posts.sort(key=lambda x: x['time'], reverse=True)

        return posts

    def get_relative_time_string(self, dt: datetime) -> str:
        """Convert datetime th√†nh string t∆∞∆°ng ƒë·ªëi"""
        now = datetime.now()
        diff = now - dt

        if diff.days > 0:
            return f"{diff.days} ng√†y tr∆∞·ªõc"
        elif diff.seconds >= 3600:
            hours = diff.seconds // 3600
            return f"{hours} gi·ªù tr∆∞·ªõc"
        elif diff.seconds >= 60:
            minutes = diff.seconds // 60
            return f"{minutes} ph√∫t tr∆∞·ªõc"
        else:
            return "V·ª´a xong"

    async def check_if_login_required(self) -> bool:
        """
        Ki·ªÉm tra xem page hi·ªán t·∫°i c√≥ y√™u c·∫ßu login kh√¥ng

        Returns:
            True n·∫øu c·∫ßn login, False n·∫øu kh√¥ng
        """
        try:
            current_url = self.page.url
            page_content = await self.page.content()

            # Ki·ªÉm tra c√°c d·∫•u hi·ªáu c·∫ßn login
            login_indicators = [
                'login' in current_url.lower(),
                'login_attempt' in current_url.lower(),
                'id="login_form"' in page_content,
                'name="login"' in page_content,
            ]

            return any(login_indicators)
        except:
            return False

    async def scrape_group(self, group_url: str, days: int = 7, skip_login: bool = False) -> List[dict]:
        """
        Main method ƒë·ªÉ scrape Facebook group

        Args:
            group_url: URL c·ªßa Facebook group
            days: S·ªë ng√†y l·∫•y posts (m·∫∑c ƒë·ªãnh 7)
            skip_login: Th·ª≠ scrape m√† kh√¥ng login (cho public groups)

        Returns:
            List c√°c posts
        """
        try:
            # Kh·ªüi t·∫°o browser
            await self.init_browser()

            # Ki·ªÉm tra login status
            is_logged_in = await self.check_login_status()

            # N·∫øu kh√¥ng skip login v√† ch∆∞a login -> y√™u c·∫ßu login
            if not skip_login and not is_logged_in:
                await self.wait_for_login()

            # N·∫øu skip login
            if skip_login and not is_logged_in:
                print(f"{Fore.CYAN}üîì Th·ª≠ truy c·∫≠p public group m√† kh√¥ng login...")

            # Navigate ƒë·∫øn group
            print(f"\n{Fore.CYAN}üåê ƒêang truy c·∫≠p group: {group_url}")
            await self.page.goto(group_url, wait_until='networkidle', timeout=60000)

            # ƒê·ª£i page load
            await asyncio.sleep(3)

            # Ki·ªÉm tra xem c√≥ b·ªã redirect v·ªÅ login page kh√¥ng
            if await self.check_if_login_required():
                print(f"{Fore.YELLOW}‚ö† Facebook y√™u c·∫ßu login ƒë·ªÉ xem group n√†y")

                if skip_login:
                    print(f"{Fore.YELLOW}üí° Group n√†y kh√¥ng ph·∫£i public ho·∫∑c c·∫ßn login ƒë·ªÉ xem")
                    print(f"{Fore.YELLOW}   B·∫°n c√≥ mu·ªën login kh√¥ng? (y/n)")
                    user_choice = input(f"{Fore.CYAN}> ").strip().lower()

                    if user_choice == 'y':
                        await self.wait_for_login()
                        # Navigate l·∫°i sau khi login
                        await self.page.goto(group_url, wait_until='networkidle', timeout=60000)
                        await asyncio.sleep(3)
                    else:
                        print(f"{Fore.RED}‚úó Kh√¥ng th·ªÉ ti·∫øp t·ª•c m√† kh√¥ng login")
                        return []

            # Scroll v√† load posts
            await self.scroll_and_load_posts(max_scrolls=50)

            # Extract post links
            posts = await self.extract_post_links(days=days)

            return posts

        except Exception as e:
            print(f"{Fore.RED}‚úó L·ªói: {str(e)}")
            raise
        finally:
            # Cleanup
            if self.context:
                await self.context.close()
            if self.playwright:
                await self.playwright.stop()

    def save_results(self, posts: List[dict], output_file: str = None):
        """
        L∆∞u k·∫øt qu·∫£ ra file

        Args:
            posts: List c√°c posts
            output_file: ƒê∆∞·ªùng d·∫´n file output (m·∫∑c ƒë·ªãnh: output/posts_{timestamp}.json)
        """
        if not posts:
            print(f"{Fore.YELLOW}‚ö† Kh√¥ng c√≥ posts n√†o ƒë·ªÉ l∆∞u")
            return

        # T·∫°o output directory
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)

        # Generate filename n·∫øu kh√¥ng c√≥
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"output/posts_{timestamp}.json"

        # L∆∞u JSON
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)

        print(f"\n{Fore.GREEN}‚úì ƒê√£ l∆∞u {len(posts)} posts v√†o: {output_file}")

        # L∆∞u c·∫£ file text ƒë∆°n gi·∫£n (ch·ªâ links)
        text_file = output_file.replace('.json', '_links.txt')
        with open(text_file, 'w', encoding='utf-8') as f:
            for post in posts:
                f.write(f"{post['link']}\n")

        print(f"{Fore.GREEN}‚úì ƒê√£ l∆∞u danh s√°ch links v√†o: {text_file}")


async def main():
    """Main function"""
    print(f"{Fore.CYAN}{'='*70}")
    print(f"{Fore.CYAN}   üì± FACEBOOK GROUP POST SCRAPER üì±")
    print(f"{Fore.CYAN}   L·∫•y t·∫•t c·∫£ links b√†i vi·∫øt m·ªõi t·ª´ Facebook group")
    print(f"{Fore.CYAN}{'='*70}\n")

    # Get input from user
    group_url = input(f"{Fore.YELLOW}üìù Nh·∫≠p link Facebook group: {Fore.WHITE}").strip()

    if not group_url:
        print(f"{Fore.RED}‚úó Vui l√≤ng nh·∫≠p link group!")
        return

    # Validate URL
    if 'facebook.com/groups' not in group_url:
        print(f"{Fore.RED}‚úó Link kh√¥ng h·ª£p l·ªá! Vui l√≤ng nh·∫≠p link Facebook group.")
        return

    days = input(f"{Fore.YELLOW}üìÖ L·∫•y posts trong bao nhi√™u ng√†y qua? (m·∫∑c ƒë·ªãnh 7): {Fore.WHITE}").strip()
    days = int(days) if days.isdigit() else 7

    # H·ªèi v·ªÅ public group
    is_public = input(f"{Fore.YELLOW}üåç Group n√†y c√≥ ph·∫£i PUBLIC group kh√¥ng? (y/n, m·∫∑c ƒë·ªãnh n): {Fore.WHITE}").strip().lower()
    skip_login = is_public == 'y'

    if skip_login:
        print(f"{Fore.CYAN}üí° S·∫Ω th·ª≠ truy c·∫≠p m√† kh√¥ng login (ch·ªâ ho·∫°t ƒë·ªông v·ªõi public groups)")
        print(f"{Fore.CYAN}   N·∫øu kh√¥ng ƒë∆∞·ª£c, tool s·∫Ω y√™u c·∫ßu login sau")

    headless_input = input(f"{Fore.YELLOW}üñ•Ô∏è  Ch·∫°y ·∫©n browser? (y/n, m·∫∑c ƒë·ªãnh n): {Fore.WHITE}").strip().lower()
    headless = headless_input == 'y'

    # Create scraper
    scraper = FacebookGroupScraper(headless=headless)

    try:
        # Scrape
        posts = await scraper.scrape_group(group_url, days=days, skip_login=skip_login)

        # Display results
        print(f"\n{Fore.CYAN}{'='*70}")
        print(f"{Fore.GREEN}üéâ K·∫æT QU·∫¢:")
        print(f"{Fore.CYAN}{'='*70}\n")

        for idx, post in enumerate(posts, 1):
            print(f"{Fore.YELLOW}{idx}. {Fore.WHITE}{post['link']}")
            print(f"   {Fore.CYAN}‚è∞ {post['time_relative']}")
            if post['preview']:
                preview = post['preview'][:100] + "..." if len(post['preview']) > 100 else post['preview']
                print(f"   {Fore.WHITE}üí¨ {preview}")
            print()

        # Save results
        scraper.save_results(posts)

        print(f"\n{Fore.GREEN}‚úì Ho√†n th√†nh!")

    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}‚ö† ƒê√£ h·ªßy b·ªüi user")
    except Exception as e:
        print(f"\n{Fore.RED}‚úó L·ªói: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
